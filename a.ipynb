{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Emergent Machine Pedagogy: A Game-Theoretic Framework\n",
    "# \n",
    "# This notebook provides a complete, end-to-end pipeline to run the experiments from the dissertation on a Google Colab T4 GPU.\n",
    "# \n",
    "# ### Instructions:\n",
    "# 1.  **Set Runtime Type:** Go to `Runtime` > `Change runtime type` and select `T4 GPU`.\n",
    "# 2.  **Upload Project:**\n",
    "#     *   On your local machine, zip your entire `cognitive-architectures` project folder. **Important:** Make sure to exclude the large `models/` directory from the zip file to keep the upload small.\n",
    "#     *   In the Colab file browser on the left, click the \"Upload to session storage\" button and upload your `cognitive-architectures.zip` file.\n",
    "# 3.  **Run All Cells:** Go to `Runtime` > `Run all`. The notebook will set up the environment, download the necessary models, and run the full experimental suite.\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ### Cell 1: Environment Setup & Project Unzip\n",
    "# \n",
    "# This cell prepares the Colab environment. It unzips the project, installs the correct GPU-accelerated dependencies, and sets up the necessary environment variables for the scripts.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# --- Unzip the project folder ---\n",
    "# This assumes you have uploaded \"cognitive-architectures.zip\" to the Colab session.\n",
    "PROJECT_ZIP_NAME = \"cognitive-architectures.zip\"\n",
    "if not os.path.exists(PROJECT_ZIP_NAME):\n",
    "    print(f\"ERROR: Please upload '{PROJECT_ZIP_NAME}' to the Colab session storage.\")\n",
    "else:\n",
    "    print(f\"Unzipping {PROJECT_ZIP_NAME}...\")\n",
    "    subprocess.run([\"unzip\", \"-q\", PROJECT_ZIP_NAME])\n",
    "    print(\"Project unzipped successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f94f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define project root and move into it ---\n",
    "PROJECT_ROOT = \"cognitive-architectures\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install GPU-accelerated dependencies ---\n",
    "# We use the requirements.colab.txt file which specifies CUDA compilation.\n",
    "print(\"\\nInstalling GPU-accelerated dependencies from requirements.colab.txt...\")\n",
    "# The CMAKE_ARGS tell llama-cpp-python to build with CUDA support (cuBLAS).\n",
    "# This is the key step for GPU acceleration.\n",
    "os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n",
    "os.environ[\"FORCE_CMAKE\"] = \"1\"\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.colab.txt\"])\n",
    "print(\"Dependencies installed successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d011e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set the PYTHONPATH ---\n",
    "# This ensures that `from src...` imports work correctly from the experiments directory.\n",
    "os.environ[\"PYTHONPATH\"] = os.getcwd()\n",
    "print(f\"PYTHONPATH set to: {os.environ['PYTHONPATH']}\")\n",
    "\n",
    "\n",
    "# --- Download the main LLM model ---\n",
    "print(\"\\nDownloading the Phi-3 GGUF model...\")\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    subprocess.run([\n",
    "        \"wget\", \n",
    "        \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "        \"-O\", MODEL_PATH\n",
    "    ])\n",
    "    print(\"Model downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Model already exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0083454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ### Cell 2: Verify GPU and Modify Code for GPU Execution\n",
    "# \n",
    "# This cell verifies that the GPU is available and makes a crucial runtime modification to the `src/common.py` file to enable full GPU offloading.\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "\n",
    "# --- Verify GPU Access ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ GPU is available. PyTorch can see the T4.\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"‚ùå WARNING: GPU not found. The experiments will be extremely slow.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Runtime Code Patch for GPU ---\n",
    "# This is a critical step. We programmatically change n_gpu_layers from 0 to -1\n",
    "# to ensure the model is fully offloaded to the T4 GPU.\n",
    "common_py_path = \"src/common.py\"\n",
    "print(f\"\\nPatching {common_py_path} for full GPU offloading...\")\n",
    "\n",
    "with open(common_py_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "new_lines = []\n",
    "patched = False\n",
    "for line in lines:\n",
    "    if \"n_gpu_layers=\" in line:\n",
    "        new_line = '    n_gpu_layers=-1, # <-- Patched by Colab notebook for full GPU offloading\\n'\n",
    "        new_lines.append(new_line)\n",
    "        print(f\"  - Found and replaced line: {line.strip()}\")\n",
    "        print(f\"  + With new line: {new_line.strip()}\")\n",
    "        patched = True\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "if patched:\n",
    "    with open(common_py_path, 'w') as f:\n",
    "        f.writelines(new_lines)\n",
    "    print(\"Patching successful.\")\n",
    "else:\n",
    "    print(\"Warning: Could not find 'n_gpu_layers' line to patch.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ### Cell 3: Run the Dry-Run Experiment\n",
    "# \n",
    "# This runs a quick, lightweight test of the entire pipeline to ensure everything is working correctly before launching the full, time-consuming experiments.\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ LAUNCHING DRY-RUN EXPERIMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# We can call the scripts directly using !python\n",
    "!python -u experiments/run_full_suite.py --task entropy --config experiments/config.yaml --mode dry-run\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ### Cell 4: Run the Full Experimental Suite\n",
    "# \n",
    "# This is the main execution cell. It runs the full, high-quality experiments for all three pedagogical tasks.\n",
    "# \n",
    "# **Estimated Runtime:** Approximately 1 to 1.5 hours on a T4 GPU.\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ LAUNCHING FULL EXPERIMENTAL SUITE\")\n",
    "print(\"This will take approximately 1 to 1.5 hours.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Run for Entropy ---\n",
    "print(\"\\n--- Running FULL experiment for ENTROPY ---\")\n",
    "!python -u experiments/run_full_suite.py --task entropy --config experiments/config.yaml --mode full\n",
    "\n",
    "# --- Run for D-Day ---\n",
    "print(\"\\n--- Running FULL experiment for D-DAY ---\")\n",
    "!python -u experiments/run_full_suite.py --task d-day --config experiments/config.yaml --mode full\n",
    "\n",
    "# --- Run for Euler's Identity ---\n",
    "print(\"\\n--- Running FULL experiment for EULERS_IDENTITY ---\")\n",
    "!python -u experiments/run_full_suite.py --task eulers_identity --config experiments/config.yaml --mode full\n",
    "\n",
    "print(\"\\nüéâ Full experimental suite complete.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ### Cell 5: Analyze Results and Display Summary\n",
    "# \n",
    "# This final cell runs the analysis script on the generated results and displays the final summary table.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä ANALYZING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Run the analysis script ---\n",
    "summary_csv_path = \"final_summary_results.csv\"\n",
    "!python -u experiments/analyze_results.py --output {summary_csv_path}\n",
    "\n",
    "\n",
    "# --- Display the final summary table ---\n",
    "print(\"\\n--- Final Aggregated Summary ---\")\n",
    "if os.path.exists(summary_csv_path):\n",
    "    summary_df = pd.read_csv(summary_csv_path)\n",
    "    # Display the full dataframe without truncation\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "        print(summary_df)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete. Results table saved to {summary_csv_path}\")\n",
    "    print(\"You can download this file from the Colab file browser.\")\n",
    "else:\n",
    "    print(f\"ERROR: Could not find the summary file at {summary_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icpo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
